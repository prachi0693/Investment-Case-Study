{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code for cleaning and analysis of the group case study on investment in course 2 of the PGDDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Style\n",
    "    - Case: \n",
    "        - snake_case for objects\n",
    "        - camelCase for functions and classes\n",
    "    - Double quotes first, then single quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used\n",
    "    - Pandas\n",
    "    - Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obejctives of Analysis\n",
    "Identify the most heavily invested main sectors in each of the three countries (for funding type FT and investments range of 5-15 M USD).\n",
    "\n",
    "Business objective: Identify the best: a. Sectors; b. Countries; c. Investment rounds for Spark Funds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "Mean amount of money invested in a particular country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Workflow\n",
    "The workflow for this analysis is rather simple. Focus on answering the questions asked in the checkpoints. Following this flow, the code in this .ipynb is organized according to the checkpoints. There will be a clear heading indicating the starting and ending of each checkpoint and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "# numpy\n",
    "import numpy as np # version: 1.15.0\n",
    "\n",
    "# pandas\n",
    "import pandas as pd # version: 0.23.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1: Data Cleaning\n",
    "There are five tasks in this checkpoint:\n",
    "    - Number of unique companies in rounds2.csv\n",
    "\t- Number of unique companies in companies.tsv\n",
    "\t- Key column from the companies dataset that can be used to merge it with rounds data\n",
    "\t- Organizations in companies that are missing in rounds2.\n",
    "    - Merge the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1.1: Importing the data\n",
    "The first step of the analysis is to import the two main datasets that we will be needing for the analysis: companies and rounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import companies.csv as companies\n",
    "companies = pd.read_csv(\"companies.txt\", sep = \"\\t\", encoding = \"ISO-8859-1\") \n",
    "\n",
    "# import rounds2.csv as rounds\n",
    "rounds2 = pd.read_csv(\"rounds2.csv\", sep = \",\", encoding = \"ISO-8859-1\")\n",
    "# ISO for lack of charset in UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#information of the companies dataset\n",
    "print(companies.info()); print(\"shape of dataset: \", companies.shape); print(\"variable dtypes:\\n\", companies.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about the rounds dataset\n",
    "print(rounds2.info()); print(\"shape of dataset: \", rounds2.shape); print(\"variable dtypes:\\n\", rounds2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1.2: Cleaning Data\n",
    "\n",
    "Since we will be focusing mostly on four variables only, let's remove all the extraneous variables from both the datasets. \n",
    "We'll remove from the companies dataset the following variables:\n",
    "    - state_code\n",
    "    - region\n",
    "    - city\n",
    "    - homepage_url\n",
    "    - founded_at\n",
    "    - name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary columns from companies\n",
    "companies.drop([\"state_code\", \"region\", \"city\", \"homepage_url\", \"founded_at\", \"name\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove the following from rounds:\n",
    "    - funded_at\n",
    "    - funding_round_code\n",
    "    - funding_round_permalink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary columns from rounds\n",
    "rounds2.drop([\"funded_at\", \"funding_round_code\", \"funding_round_permalink\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q1: Number of unique companies in rounds\n",
    "To do this, we'll use the company_permalink column. However, instead of doing this directly, we'll first convert the company_permalink to lowercase and then determine the number of unique records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting company_permalink to lower case and getting number of unique records.\n",
    "rounds2.company_permalink.str.lower().nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be 66370 unique companies in the dataset. This means that there are companies that had more than one round of funding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checpoint 1 Q2: Number of unique companies in companies\n",
    "This time, we'll use the permalink, which is supposed to be the UID of a company. As with rounds.company_permalink, we'll first convert to lower case and then proceed to count the number of unique records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.permalink.str.lower().nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The companies dataset has 66368 unique companies. There seems to be a discrepancy between the number of unique records in companies and rounds. Does this mean that there are at least 2 companies in rounds that are not present in companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q3: Key column to merge companies and rounds\n",
    "From the data dictionary we know the companies.permalink and rounds.company_permalink are UID's of each company in the dataset. So, we'll use companies.permalink as the key to merge with rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q4: Mismatches between rounds and companies\n",
    "Ok. Now, we're required to find out if there are any records that are unique to rounds only. That is these organizations are not present in companies but are present in rounds.\n",
    "\n",
    "We can do this by merging on companies.permalink and rounds.company_permalink. But, we'll take a slightly different approach here. \n",
    "\n",
    "Instead, the same result can be achieved by checking if every permalink in rounds.company_permalink is present in company.permalink. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting rounds.company_permalink and companies.permalink to lower case\n",
    "companies[\"permalink\"] = companies.permalink.str.lower()\n",
    "\n",
    "rounds2[\"company_permalink\"] = rounds2.company_permalink.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any unique records.\n",
    "print(rounds2.company_permalink[~rounds2.company_permalink.isin(companies.permalink)].dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be 7 companies that are in rounds but not in companies. So, the answer to the fourth question is yes. There are organizations that are present in rounds but not in companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q5: Merge the two dataframes\n",
    "This is the basis of all our analysis. Merging the two DataFrames will give us a single data frame which contains all the data needed. Let's get the cursory look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(rounds2, companies, how = \"inner\", left_on = \"company_permalink\", \n",
    "                  right_on = \"permalink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unnecessary rows\n",
    "\n",
    "So, after some thought it was decided unanimously that the best approach would be to drop all missing values from the raised_amount_usd column. This decision was made because imputing values in the target column might lead to biased results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all missing values from the rounds DataFrame.\n",
    "rounds2.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds2.shape # new shape of rounds: (94959, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping those missing values leaves us with 95K observations, which is not a problem as we still left with > 75% of the observations. The next step is to combine the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the rounds2 and companies data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_frame = pd.merge(rounds2, companies, how = \"inner\", left_on = \"company_permalink\", \n",
    "                  right_on = \"permalink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "master_frame.isnull().mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two variables with missing values: country_code (an important variable) has 6% values missing and category_list has 1% values missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is to check for duplicates. If there are duplicates, just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates in the master DataFrame\n",
    "master_frame.duplicated().sum() # there seem to be 1311 duplicated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_frame.drop_duplicates(keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, dropping duplicates resulted in the loss of about 2K rows. We're now left with about 93K rows. This is about 81% of the initial observations. Deleting any more values is a good idea. This would result in the loss of more data, bringing the total down even more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating the remaining missing values\n",
    "Since the remaining missing values are present in series of dtype objects. This means that instead of just deleting the values, we can flag them, there by retaining values in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing missing values in country_code with new sentinel value\n",
    "master_frame[\"country_code\"].fillna(\"unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on running the following code, we see that the missing value percentage has gone down\n",
    "master_frame.isnull().mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have about 1% of the total values missing 1% of it's data. 1% of 93647 is about 940. We can go ahead and delete these rows. We'll still be left with 80.5% of the total observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_frame.dropna(inplace = True) # Now that we have this DataFrame, we can finally start with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the information about the master table after performing all the operations\n",
    "print(master_frame.info()); print(master_frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2: Funding Type Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 2.1 Investment Analysis\n",
    "### Change the unit of 'raised_amount_usd' column\n",
    "\n",
    "The investment is in terms of 'million USD', convert the unit of the `raised_amount_usd` from `$` to `million $`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting raised_amount_usd from dollars to millions of dollars would increase the readability of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for unit conversion here\n",
    "master_frame[\"raised_amount_usd\"] = master_frame[\"raised_amount_usd\"] / 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the average investment amount for each of the four funding types (venture, angel, seed, and private equity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_frame.groupby(\"funding_round_type\").raised_amount_usd.mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q: \n",
    "### Table 2.1: Average Values of Investments for Each of these Funding Types \n",
    " Average funding amount of venture type: 11.682595 (million USD) \t                              \n",
    " Average funding amount of angel type: 0.961039 (million USD)\t \n",
    " Average funding amount of seed type: 0.721313 (million USD)\t \n",
    " Average funding amount of private equity type: 73.350449 (million USD)\t \n",
    "\n",
    "Considering that Spark Funds wants to invest between 5 to 15 million USD per investment round, which investment type is the most suitable for it?\n",
    "\n",
    "venture\n",
    "\t                                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 3: Country Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Subtask 3.1: Find the top nine countries with highest total funding for the given investment = \"venture\"\n",
    "\n",
    "    -- Spark Funds wants to see the top nine countries which have received the highest total funding (across ALL sectors for the chosen investment type)\n",
    "\n",
    "    -- For the chosen investment type, make a data frame named top9 with the top nine countries (based on the total investment amount each country has received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby command here to find the country-wise total funding for the investment type \n",
    "master_grpby_country= master_frame.loc[master_frame.funding_round_type == \"venture\", :].groupby(\"country_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to find the top nine countries with highest total funding\n",
    "top9 = master_grpby_country[\"raised_amount_usd\"].sum().sort_values(ascending=False)[:9]\n",
    "print(top9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 3.2: Identify the top three English-speaking countries in the data frame top9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3.1: Analysing the Top 3 English-Speaking Countries\n",
    "Based on the list of countries where English is an official language - the top three English-speaking countries are:\n",
    "\n",
    " 1. Top English-speaking country:    USA (United States)             \n",
    " 2. Second English-speaking country: GBR (United Kingdom)\n",
    " 3. Third English-speaking country:  IND (India)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 4: Sector Analysis 1\n",
    "So, let's start off with the sector analysis. Here's what needs to be done in this part of the analysis.\n",
    "    - Extract the primary sector of each category list from the category_list column.\n",
    "    - Use the mapping file 'mapping.csv' to map each primary sector to one of the eight main sectors (Note that ‘Others’ is also considered one of the main sectors)\n",
    "\n",
    "The primary sector is the string that appears before the first pipe \"|\" in the category_list variable. So, let's get on with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 4.1 Extracting the primary sector from category_list\n",
    "The first step is to create a new variable called primary_sector to store the primary sector of each organization in the dataframe.\n",
    "\n",
    "The primary sector is the first string that results from splitting master.category_list on \"|\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the primary sector\n",
    "master_frame[\"primary_sector\"] = master_frame.category_list.str.split(\"|\").str.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to map each primary sector in master.primary_sector to a main sector. This is in the mapping.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 4.2 Map each primary sector to one of the eight main sectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading mapping.csv as mapping\n",
    "mapping = pd.read_csv(\"mapping.csv\", sep = \",\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic information about mapping\n",
    "print(mapping.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the Blanks column since this just serves as a flag to identify NaN's\n",
    "mapping.drop(\"Blanks\", axis = 1, inplace = True)\n",
    "\n",
    "# dropping the single NaN at the head of the dataset\n",
    "mapping.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the final steps to finish up checkpoint 4.\n",
    "1. Merge master and mapping_tidy\n",
    "2. Check for: a. Missing values; b. Duplicates\n",
    "3. Treat missing values and drop duplicates`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mapping file has turned out to be a one-to-one sparse matrix that maps each primary sector to one of the eight main sectors. \n",
    "The main task here is this: convert the wide (and sparse) representation of the mapping into a long representation. This means bringing in all the colums under one roof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before this is done though, a check to see if every there are any records present in master.primary_sector are not in mapping.category_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary sectors present in master.primary_sector but not present in mapping_tidy.category_list\n",
    "master_frame.primary_sector[~master_frame.primary_sector.isin(mapping.category_list)].dropna().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result of the code above, it's clear that there are 89 sub-sectors that are not included in the mapping dataset. To deal with this issue, the sectors have been manually assigned a main_sector by observation.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are primary sectors that are present in master.primary_sector but not in mapping.category_list.\n",
    "    - Analytics \n",
    "    - Finance \n",
    "    - Financial Services \n",
    "    - Finance Technology \n",
    "    - Business Analytics \n",
    "    - Big Data Analytics \n",
    "    - Investment Management \n",
    "    - Social Media Advertising \n",
    "    - Personal Finance \n",
    "    - Predictive Analytics \n",
    "    - Financial Exchanges\n",
    "    - Mobile Analytics \n",
    "    - Social Media Management\n",
    "    - Promotional\n",
    "    - Waste Management\n",
    "    - Natural Gas Uses\n",
    "    - Biotechnology and Semiconductor\n",
    "    - Green Tech\n",
    "    - Energy Management\n",
    "    - Natural Resources\n",
    "    - Alternative Medicine\n",
    "    - Cannabis\n",
    "    - Medical Professionals\n",
    "    - Personal Health\n",
    "    - Mobile Emergency&Health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On closer inspection of the mapping dataframe, it was seen that there are some misspelt words in mapping.category_list. And, most of them are just misspelt words in the list written above. This will be the approach to fix the problem:\n",
    "1. Get an idea of the words that are misspelt and their number\n",
    "2. Replace them with their correct spellings in the mapping dataframe\n",
    "3. Merge with the master dataframe\n",
    "4. Treat any missing values that might arise because of the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories in mapping but not in master.primary_sector\n",
    "print(mapping.category_list[~mapping.category_list.isin(master_frame.primary_sector)].dropna().unique());\n",
    "print(mapping.category_list[~mapping.category_list.isin(master_frame.primary_sector)].dropna().unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 61 words that need to be corrected. Instead of correcting each mis-spelling separately, it is more efficient to write a function to do that. Then, a for loop and a list of words can be used to correct each spelling mistake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace missplet words\n",
    "def wordReplacer(wrong_spelling, correct_spelling):\n",
    "    \"\"\" This function takes in two parameters: the wrong_spelling and the correct_spelling.\n",
    "    It returns mapping.category_list with the modified spellings.\n",
    "    \"\"\"\n",
    "    return mapping[\"category_list\"].str.replace(pat = wrong_spelling, repl = correct_spelling, regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words that need to be replaced\n",
    "list_of_words = [[\"Alter0tive\", \"Alternative\"], [\"A0lytics\", \"Analytics\"], [\"Ma0gement\", \"Management\"], \n",
    "                 [\"Can0bis\", \"Cannabis\"], [\"Fi0nce\", \"Finance\"], [\"Sig0ge\", \"Signage\"], \n",
    "                 [\"Fi0ncial\", \"Financial\"], [\"Gover0nce\", \"Governance\"], [\"Jour0lism\", \"Journalism\"], \n",
    "                 [\"Professio0ls\", \"Professionals\"], [\"0notechnology\", \"Nanotechnology\"], \n",
    "                 [\"0tural\", \"Natural\"], [\"0vigation\", \"Navigation\"], [\"Perso0l\", \"Personal\"],\n",
    "                 [\"Perso0lization\", \"Personalization\"], [\"Professio0l\", \"Professional\"], \n",
    "                 [\"Promotio0l\", \"Promotional\"], [\"Veteri0ry\", \"Veterinary\"], [\"Chi0\", \"China\"], \n",
    "                 [\"Educatio0l\", \"Educational\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the misspelt words with the correct spelling\n",
    "for i in list_of_words:\n",
    "    mapping[\"category_list\"] = wordReplacer(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing categories again.\n",
    "master_frame.primary_sector[~master_frame.primary_sector.isin(mapping.category_list)].dropna().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There still are some primary_sectors missing. This can be solved by creating bins for each of the missing sectors and assigning them to a main sector manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists of primary_sectors that fall under a main sector\n",
    "\n",
    "# Cleantech / Semiconductors\n",
    "cleantech_semiconductors = [\"Natural Gas Uses\", \"Biotechnology and Semiconductor\", \"GreenTech\"]\n",
    "\n",
    "# Health\n",
    "health = [\"Mobile Emergency&Health\", \"Psychology\"]\n",
    "\n",
    "# Entertainment\n",
    "entertainment = [\"Internet TV\", \"Skill Gaming\", \"Racing\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathering all the columns under one roof\n",
    "mapping_long = pd.melt(mapping, id_vars = [\"category_list\"], var_name = \"main_sector\", value_name = \"yes_no\")\n",
    "\n",
    "# tidying mapping_long to produce the final version of the mapping dataset\n",
    "mapping_tidy = mapping_long.loc[mapping_long.yes_no == 1, [\"category_list\", \"main_sector\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up, let's take one look at the mapping_tidy dataset just to make sure everything is alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping_tidy.info());\n",
    "print(mapping_tidy.isnull().mean()) # no null values. We can proceed to merge the two datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 4.3: Generate the merged data frame with each primary sector mapped to its main sector\n",
    "\n",
    "These will be the final steps to get the merged dataframe:\n",
    "1. Merge the mapping_tidy dataset with the master dataset using an left join\n",
    "2. Impute the missing main_sector values in the merged master dataset\n",
    "3. Replace the other missing values in main_sector with the \"Others\" flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the master and mapping_tidy\n",
    "master_frame = pd.merge(master_frame, mapping_tidy, how = \"left\", left_on = \"primary_sector\", right_on = \"category_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the category_list_y variable and renaming category_list_x as category_list\n",
    "master_frame.drop(\"category_list_y\", axis = 1, inplace = True)\n",
    "\n",
    "master_frame.rename(index = str, columns = {\"category_list_x\": \"category_list\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling up the main_sector with appropriate values.\n",
    "# social, analytics, finance, advertising\n",
    "master_frame.loc[master_frame.primary_sector.isin([\"Social Media Advertising\"]), \"main_sector\"] = \\\n",
    "                                                    \"Social, Finance, Analytics, Advertising\"\n",
    "\n",
    "# cleantech / semiconductors\n",
    "master_frame.loc[master_frame.primary_sector.isin(cleantech_semiconductors), \"main_sector\"] = \"Cleantech / Semiconductors\"\n",
    "\n",
    "# health\n",
    "master_frame.loc[master_frame.primary_sector.isin(health), \"main_sector\"] = \"Health\"\n",
    "\n",
    "# filling up the remaining missing values with others\n",
    "master_frame.loc[master_frame.main_sector.isnull(), \"main_sector\"] = \"Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the dataset can be used to for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export this dataframe to work in Tableau for Checkpoint 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_frame.to_csv('master_file.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 5: Sector Analysis 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to find out the most heavily invested main sectors in each of the three countries (for funding type FT and investments range of 5-15 M USD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to obtain the deliverables of this checkpoint:\n",
    "1. Create a dataframe for each country with the preferred funding type.\n",
    "2. Add the total investements and counts of investements in each sector to the dataframes.\n",
    "3. Fill out the table with the results we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 5.1: Create a dataframe for each country with the preferred funding type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the three dataframes\n",
    "# usa\n",
    "D1 = master_frame.loc[(master_frame.country_code == \"USA\") & (master_frame.funding_round_type == \"venture\"), :]\n",
    "\n",
    "# great britain\n",
    "D2 = master_frame.loc[(master_frame.country_code == \"GBR\") & (master_frame.funding_round_type == \"venture\"), :]\n",
    "\n",
    "# india\n",
    "D3 = master_frame.loc[(master_frame.country_code == \"IND\") & (master_frame.funding_round_type == \"venture\"), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 5.2: Add the total investements and counts of investements in each sector to the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the total investements and adding them to the dataframes\n",
    "#usa\n",
    "D1_summary = D1.groupby(\"main_sector\").raised_amount_usd.agg([\"sum\", \"count\"])\n",
    "\n",
    "D1 = pd.merge(D1, D1_summary, how = \"left\", on = \"main_sector\")\n",
    "\n",
    "# great britain\n",
    "D2_summary = D2.groupby(\"main_sector\").raised_amount_usd.agg([\"sum\", \"count\"])\n",
    "\n",
    "D2 = pd.merge(D2, D2_summary, how = \"left\", on = \"main_sector\")\n",
    "\n",
    "# india\n",
    "D3_summary = D3.groupby(\"main_sector\").raised_amount_usd.agg([\"sum\", \"count\"])\n",
    "\n",
    "D3 = pd.merge(D3, D3_summary, how = \"left\", on = \"main_sector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataframes, it's time to answer the questions asked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 5.3: Fill out the table with the results.\n",
    "### Table 5.1 : Sector-wise Investment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of investments in countries\n",
    "# usa\n",
    "print(\"Number of investments made in USA:\", D1.shape[0])\n",
    "\n",
    "# great britain\n",
    "print(\"Number of investements made in Great Britain:\", D2.shape[0])\n",
    "\n",
    "# india\n",
    "print(\"Number of investments made in India:\", D3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Total number of investments (count) in each country:\n",
    "USA: 35292\n",
    "GBR: 2027\n",
    "IND: 813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total size of investments in countries\n",
    "top3 = master_frame.loc[(master_frame.country_code.isin([\"USA\", \"GBR\", \"IND\"])) & (master_frame.funding_round_type == \"venture\"), :]\n",
    "top3.groupby(\"country_code\").raised_amount_usd.agg([\"sum\", \"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Total amount of investment (USD) in each country:\n",
    "USA: 411102.768986(million USD)\n",
    "GBR: 19931.867246(million USD)\n",
    "IND: 14134.008718(million USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top three sectors based on count of investments\n",
    "# usa\n",
    "print(\"USA\", \"\\n\", D1.groupby(\"main_sector\").raised_amount_usd.count().sort_values(ascending = False), \"\\n\")\n",
    "\n",
    "# great britain\n",
    "print(\"Great Britain\", \"\\n\", D2.groupby(\"main_sector\").raised_amount_usd.count().sort_values(ascending = False), \n",
    "      \"\\n\")\n",
    "\n",
    "# india\n",
    "print(\"India\", \"\\n\", D3.groupby(\"main_sector\").raised_amount_usd.count().sort_values(ascending = False), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Top sector (based on count of investments):\n",
    "USA: Others\n",
    "GBR: Others\n",
    "IND: Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Second-best sector (based on count of investments):\n",
    "USA: Cleantech / Semiconductors\n",
    "GBR: Cleantech / Semiconductors\n",
    "IND: Social, Finance, Analytics, Advertising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Third-best sector (based on count of investments):\n",
    "USA: Social, Finance, Analytics, Advertising\n",
    "GBR: Social, Finance, Analytics, Advertising\n",
    "IND: News, Search and Messaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Number of investments in the top sector (refer to point 3):\n",
    "USA:8521\n",
    "GBR:526\n",
    "IND:285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Number of investments in the second-best sector (refer to point 4):\n",
    "USA:7723\n",
    "GBR:436\n",
    "IND:144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Number of investments in the third-best sector (refer to point 5):\n",
    "USA:6984\n",
    "GBR:414\n",
    "IND:139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the top sector count-wise (point 3), which company received the highest investment?\n",
    "# usa\n",
    "print(\"USA\", \n",
    "      D1[D1.main_sector == \"Others\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values(ascending = False)[:10],\n",
    "     \"\\n\")\n",
    "\n",
    "# gbr\n",
    "print(\"Great Britain\", \n",
    "      D2[D2.main_sector == \"Others\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values(ascending = False)[:10],\n",
    "     \"\\n\")\n",
    "\n",
    "# ind\n",
    "print(\"India\", \n",
    "      D3[D3.main_sector == \"Others\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values(ascending = False)[:10],\n",
    "     \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. For the top sector count-wise (point 3), which company received the highest investment?\n",
    "USA: social-finance \n",
    "GBR: oneweb\n",
    "IND: flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the second-best sector which company received the highest investment?\n",
    "# usa\n",
    "print(\"USA\", \"\\n\",\n",
    "      D1[D1.main_sector == \"Cleantech / Semiconductors\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values(ascending = False)[:10],\n",
    "     \"\\n\")\n",
    "\n",
    "# gbr\n",
    "print(\"Great Britain\", \"\\n\",\n",
    "      D2[D2.main_sector == \"Cleantech / Semiconductors\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values(ascending = False)[:10],\n",
    "     \"\\n\")\n",
    "\n",
    "# ind\n",
    "print(\"India\", \"\\n\",\n",
    "      D3[D3.main_sector == \"Social, Finance, Analytics, Advertising\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values(ascending = False)[:10],\n",
    "     \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. For the second-best sector count-wise (point 4), which company received the highest investment?\n",
    "USA: Freescale\n",
    "GBR: ImmunoCore\n",
    "IND: Shopclues.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
